{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Clasificación-y-principales-características\" data-toc-modified-id=\"Clasificación-y-principales-características-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Clasificación y principales características</a></span></li><li><span><a href=\"#Objetivo\" data-toc-modified-id=\"Objetivo-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Objetivo</a></span></li><li><span><a href=\"#Funcionamiento-del-algoritmo\" data-toc-modified-id=\"Funcionamiento-del-algoritmo-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Funcionamiento del algoritmo</a></span></li><li><span><a href=\"#Decision-trees-en-Scikit-Learn\" data-toc-modified-id=\"Decision-trees-en-Scikit-Learn-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Decision trees en Scikit-Learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Clasificación\" data-toc-modified-id=\"Clasificación-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Clasificación</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hiperparámetros\" data-toc-modified-id=\"Hiperparámetros-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Hiperparámetros</a></span></li></ul></li><li><span><a href=\"#Regresión\" data-toc-modified-id=\"Regresión-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Regresión</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hiperparámetros\" data-toc-modified-id=\"Hiperparámetros-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Hiperparámetros</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación y principales características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de árbol de decisión es:\n",
    "- Apropiado para tareas tanto de clasificación, como de regresión. \n",
    "- Supervisado: requiere datos de entrenamiento etiquetados.\n",
    "- De aprendizaje basado en modelo.\n",
    "- De tipo \"eager learning\": durante el entrenamiento se construye un modelo que se utiliza después para hacer las predicciones. \n",
    "- No paramétrico: no requiere que la distribución de la población sea caracterizada por ciertos parámetros.\n",
    "- Global: se construye un modelo, no se tienen en cuenta únicamente las instancias de entrenamiento más cercanas a las nuevas muestras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los árboles de decisión (decision trees) son un método de aprendizaje supervisado no paramétrico que se utiliza para clasificación y regresión. El objetivo es crear un modelo que prediga el valor de una variable objetivo mediante el aprendizaje de reglas de decisión simples inferidas a partir de las otras variables. Cuanto más profundo es el árbol, más complejas son las reglas de decisión y más ajustado es el modelo.\n",
    "\n",
    "Los árboles de decisión se pueden utilizar tanto para problemas de clasificación, como para problemas de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcionamiento del algoritmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varios algoritmos que permiten generar árboles de decisión:\n",
    "- ID3\n",
    "- C4.5\n",
    "- C5.0\n",
    "- CART\n",
    "\n",
    "Scikit-Learn utiliza una versión de CART optimizada.\n",
    "\n",
    "Las medidas de impureza que se suelen utilizar son:\n",
    "- Para clasificación, la impureza de Gini y la entropía. https://scikit-learn.org/stable/modules/tree.html#classification-criteria\n",
    "\n",
    "- Para regresión, el mse, el friedman-mse y el mae. https://scikit-learn.org/stable/modules/tree.html#regression-criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees en Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase *DecisionTreeClassifier* de Scikit-Learn es capaz de abordar problemas tanto de clasificación binaria como multiclase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.tree.DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**criterion**\n",
    "\n",
    "Permite indicar la función para medir la calidad de una división. \n",
    "\n",
    "Puede ser una de las siguientes opciones:\n",
    "    \n",
    "{\"gini\", “entropy”}\n",
    "\n",
    "Por defecto es ”gini”.\n",
    "\n",
    "- \"gini\" usa la impureza de Gini.\n",
    "- “entropy” usa la ganancia de información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**splitter**\n",
    "\n",
    "Es la estrategia utilizada para elegir la división en cada nodo. \n",
    "\n",
    "Puede ser una de las siguientes opciones:\n",
    "    \n",
    "{“best”, “random”}\n",
    "\n",
    "Por defecto es ”best”.\n",
    "\n",
    "- \"best\" elige la mejor división.\n",
    "- \"random\" elige la mejor división aleatoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_depth**\n",
    "\n",
    "Es la profundidad máxima del árbol. Si es *None*, los nodos se expanden hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de *min_samples_split* muestras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_samples_split**\n",
    "\n",
    "Es el número mínimo de muestras necesarias para dividir un nodo interno:\n",
    "\n",
    "- Si es un valor de tipo int, entonces este es el número mínimo.\n",
    "\n",
    "- Si es un valor de tipo float, entonces min_samples_split es una fracción y el número mínimo de muestras para cada división viene dado por ceil(min_samples_split * n_samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_samples_leaf**\n",
    "\n",
    "Es el número mínimo de muestras necesarias para estar en un nodo hoja. Un punto de división a cualquier profundidad solo se considerará si deja al menos *min_samples_leaf* muestras de entrenamiento en cada una de las ramas izquierda y derecha. Esto puede tener el efecto de suavizar el modelo, especialmente en regresión.\n",
    "\n",
    "- Si es un valor de tipo int, entonces este es el número mínimo.\n",
    "\n",
    "- Si es un valor de tipo float, entonces min_samples_leaf es una fracción y el número mínimo de muestras para cada nodo viene dado por ceil(min_samples_leaf * n_samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_weight_fraction_leaf**\n",
    "\n",
    "Es la fracción ponderada mínima de la suma total de pesos (de todas las muestras de entrada) que se requiere para estar en un nodo hoja. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_features**\n",
    "\n",
    "Es el número de variables a considerar cuando se busca la mejor división. \n",
    "\n",
    "Puede ser de tipo int, float o una opción de las siguientes:\n",
    "\n",
    "{“auto”, “sqrt”, “log2”}\n",
    "\n",
    "Por defecto es None.\n",
    "\n",
    "- Si es de tipo int, entonces se consideran *max_features* variables en cada división.\n",
    "- Si es de tipo float, entonces *max_features* es una fracción y en cada división se considera un número de variables dado por int(max_features * n_features).\n",
    "- Si es “auto”, entonces max_features=sqrt(n_features).\n",
    "- Si es “sqrt”, entonces max_features=sqrt(n_features).\n",
    "- Si es “log2”, entonces max_features=log2(n_features).\n",
    "- Si es None, entonces max_features=n_features.\n",
    "\n",
    "Nota: la búsqueda de una división no se detiene hasta que se encuentra al menos una partición válida de las muestras de nodos, incluso si requiere inspeccionar de manera efectiva más de *max_features* características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max_leaf_nodes**\n",
    "\n",
    "Indica el número total de nodos hoja en un árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_impurity_decrease**\n",
    "\n",
    "Un nodo se dividirá si esta división induce una disminución de la impureza mayor o igual al valor dado por este hiperparámetro.\n",
    "\n",
    "La ecuación ponderada de disminución de impurezas es la siguiente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min_impurity_split**\n",
    "\n",
    "Es el umbral de parada temprana (early stopping) en el crecimiento de los árboles. Un nodo se dividirá si su impureza está por encima del umbral; de lo contrario, será un nodo hoja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**class_weight**\n",
    "\n",
    "Pesos asociados a las clases, dados en forma {class_label: weight}. Si es *None*, se supone que todas las clases tienen peso 1. Para problemas multi-output, se puede proporcionar una lista de diccionarios en el mismo orden que las columnas de la variable *y*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ccp_alpha**\n",
    "\n",
    "Hiperparámetro de complejidad utilizado para la poda de mínimo coste-complejidad (Minimal Cost-Complexity Pruning). Se elegirá el subárbol con mayor complejidad de coste, que a su vez sea más pequeño que el valor espedificado mediante ccp_alpha. De forma predeterminada, no se realiza ninguna poda. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.tree.DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort='deprecated', ccp_alpha=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**criterion**\n",
    "\n",
    "Permite indicar la función para medir la calidad de una división. \n",
    "\n",
    "Puede ser una de las siguientes opciones:\n",
    "    \n",
    "{“mse”, “friedman_mse”, “mae”}\n",
    "\n",
    "Por defecto es ”mse”.\n",
    "\n",
    "-  \"mse\" usa el error cuadrático medio, que es igual a la reducción de la varianza como criterio de selección de características y minimiza la pérdida L2 utilizando la media de cada nodo terminal.\n",
    "- \"friedman_mse\" usa el error cuadrático medio con la puntuación de mejora de Friedman para el potencial split\n",
    "- \"mae\" usa el error absoluto medio, lo que minimiza la pérdida de L1 utilizando la mediana de cada nodo terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resto de hiperparámetros**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos los demás hiperparámetros son iguales que los que se usan en clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
