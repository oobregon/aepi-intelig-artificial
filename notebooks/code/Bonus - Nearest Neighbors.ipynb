{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Clasificación-y-principales-características\" data-toc-modified-id=\"Clasificación-y-principales-características-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Clasificación y principales características</a></span></li><li><span><a href=\"#Objetivo\" data-toc-modified-id=\"Objetivo-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Objetivo</a></span></li><li><span><a href=\"#Métricas-de-distancias\" data-toc-modified-id=\"Métricas-de-distancias-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Métricas de distancias</a></span><ul class=\"toc-item\"><li><span><a href=\"#Distancia-euclídea\" data-toc-modified-id=\"Distancia-euclídea-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Distancia euclídea</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ejemplo-en-2D\" data-toc-modified-id=\"Ejemplo-en-2D-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Ejemplo en 2D</a></span></li><li><span><a href=\"#Ejemplo-en-3D\" data-toc-modified-id=\"Ejemplo-en-3D-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>Ejemplo en 3D</a></span></li></ul></li><li><span><a href=\"#Distancia-Manhattan\" data-toc-modified-id=\"Distancia-Manhattan-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Distancia Manhattan</a></span></li><li><span><a href=\"#Distancia-Minkowski\" data-toc-modified-id=\"Distancia-Minkowski-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Distancia Minkowski</a></span><ul class=\"toc-item\"><li><span><a href=\"#Equivalencia-con-distancia-Manhattan\" data-toc-modified-id=\"Equivalencia-con-distancia-Manhattan-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>Equivalencia con distancia Manhattan</a></span></li><li><span><a href=\"#Equivalencia-con-distancia-euclídea\" data-toc-modified-id=\"Equivalencia-con-distancia-euclídea-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>Equivalencia con distancia euclídea</a></span></li></ul></li></ul></li><li><span><a href=\"#KNN-en-Scikit-Learn\" data-toc-modified-id=\"KNN-en-Scikit-Learn-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>KNN en Scikit-Learn</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vecinos-más-cercanos-sin-supervisión\" data-toc-modified-id=\"Vecinos-más-cercanos-sin-supervisión-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Vecinos más cercanos sin supervisión</a></span><ul class=\"toc-item\"><li><span><a href=\"#Algoritmo-de-fuerza-bruta\" data-toc-modified-id=\"Algoritmo-de-fuerza-bruta-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Algoritmo de fuerza bruta</a></span></li><li><span><a href=\"#Algoritmo-KDTree\" data-toc-modified-id=\"Algoritmo-KDTree-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Algoritmo KDTree</a></span></li><li><span><a href=\"#Algoritmo-BallTree\" data-toc-modified-id=\"Algoritmo-BallTree-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Algoritmo BallTree</a></span></li></ul></li><li><span><a href=\"#Vecinos-más-cercanos-con-supervisión\" data-toc-modified-id=\"Vecinos-más-cercanos-con-supervisión-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Vecinos más cercanos con supervisión</a></span><ul class=\"toc-item\"><li><span><a href=\"#Problemas-de-clasificación\" data-toc-modified-id=\"Problemas-de-clasificación-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Problemas de clasificación</a></span></li><li><span><a href=\"#Problemas-de-regresión\" data-toc-modified-id=\"Problemas-de-regresión-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Problemas de regresión</a></span></li></ul></li></ul></li><li><span><a href=\"#Hiperparámetros\" data-toc-modified-id=\"Hiperparámetros-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Hiperparámetros</a></span><ul class=\"toc-item\"><li><span><a href=\"#Modelo-basados-en-k-vecinos-más-cercanos\" data-toc-modified-id=\"Modelo-basados-en-k-vecinos-más-cercanos-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Modelo basados en k vecinos más cercanos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hiperparámetro-n_neighbors\" data-toc-modified-id=\"Hiperparámetro-n_neighbors-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Hiperparámetro <em>n_neighbors</em></a></span></li><li><span><a href=\"#Hiperparámetro-weights\" data-toc-modified-id=\"Hiperparámetro-weights-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Hiperparámetro <em>weights</em></a></span></li><li><span><a href=\"#Hiperparámetro-algorithm\" data-toc-modified-id=\"Hiperparámetro-algorithm-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Hiperparámetro <em>algorithm</em></a></span></li><li><span><a href=\"#Hiperparámetro-leaf_size\" data-toc-modified-id=\"Hiperparámetro-leaf_size-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Hiperparámetro <em>leaf_size</em></a></span></li><li><span><a href=\"#Hiperparámetro-p\" data-toc-modified-id=\"Hiperparámetro-p-5.1.5\"><span class=\"toc-item-num\">5.1.5&nbsp;&nbsp;</span>Hiperparámetro <em>p</em></a></span></li><li><span><a href=\"#Hiperparámetro-metric\" data-toc-modified-id=\"Hiperparámetro-metric-5.1.6\"><span class=\"toc-item-num\">5.1.6&nbsp;&nbsp;</span>Hiperparámetro <em>metric</em></a></span></li><li><span><a href=\"#Hiperparámetro-metric_params\" data-toc-modified-id=\"Hiperparámetro-metric_params-5.1.7\"><span class=\"toc-item-num\">5.1.7&nbsp;&nbsp;</span>Hiperparámetro <em>metric_params</em></a></span></li></ul></li><li><span><a href=\"#Modelo-basados-en-un-radio-de-vecinos-más-cercanos\" data-toc-modified-id=\"Modelo-basados-en-un-radio-de-vecinos-más-cercanos-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Modelo basados en un radio de vecinos más cercanos</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hiperparámetro-radius\" data-toc-modified-id=\"Hiperparámetro-radius-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Hiperparámetro radius</a></span></li><li><span><a href=\"#Hiperparámetro-outlier_label\" data-toc-modified-id=\"Hiperparámetro-outlier_label-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Hiperparámetro <em>outlier_label</em></a></span></li><li><span><a href=\"#Resto-de-hiperparámetros\" data-toc-modified-id=\"Resto-de-hiperparámetros-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Resto de hiperparámetros</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación y principales características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo KNN es:\n",
    "- De clasificación, principalmente, aunque también se adapta a problemas de regresión: si las etiquetas de los datos de entrenamiento son variables continuas en lugar de variables discretas, la etiqueta asignada a un nuevo punto se calcula en función de la media de las etiquetas de sus k vecinos más cercanos. \n",
    "- Supervisado: requiere datos de entrenamiento etiquetados.\n",
    "- De aprendizaje basado en instancias.\n",
    "- De tipo \"perezoso\" (lazy): durante el entrenamiento simplemente guarda las instancias, no construye ningún modelo (a diferencia de, por ejemplo, los árboles de decisión). \n",
    "- No paramétrico: no hace suposiciones sobre la distribución que siguen los datos (a diferencia de, por ejemplo, un modelo lineal).\n",
    "- Local: asume que la clase de un dato depende solo de los k vecinos más cercanos (no se construye un modelo global)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El objetivo de KNN es encontrar el número (predefinido) de muestras de entrenamiento más cercanas en distancia a un nuevo punto y predecir su etiqueta a partir de ellas. \n",
    "\n",
    "El algoritmo KNN asume que los puntos que se encuentran próximos unos de otros serán, a su vez, similares entre sí. Por tanto, depende de que esta suposición sea lo suficientemente cierta como para que el algoritmo sea útil. KNN captura la idea de similitud (a veces llamada distancia, proximidad o cercanía) con una base sencilla de matemáticas: calcular la distancia entre puntos en un gráfico. Hay muchas formas de calcular la distancia y alguna en concreto puede ser preferible frente a otras, según el problema que estemos tratando de resolver:\n",
    "\n",
    "- Distancia euclídea\n",
    "- Distancia de Manhattan\n",
    "- Distancia de Minkowski\n",
    "\n",
    "En general, la distancia euclidiana estándar es la opción más común.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de distancias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distancia euclídea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo en 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = DistanceMetric.get_metric('euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1, 2],\n",
    "    [6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 7.07106781],\n",
       "       [7.07106781, 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.pairwise(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo en 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[4, 2, 3],\n",
    "    [1, 4, 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.12310563],\n",
       "       [4.12310563, 0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.pairwise(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distancia Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = DistanceMetric.get_metric('manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[3, 3],\n",
    "    [10 ,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., 12.],\n",
       "       [12.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.pairwise(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distancia Minkowski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalencia con distancia Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import DistanceMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_manhattan = DistanceMetric.get_metric('minkowski', p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[3, 3],\n",
    "    [10 ,8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., 12.],\n",
       "       [12.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El resultado es idéntico al obtenido utilizando la métrica de la distancia Manhattan\n",
    "dist_manhattan.pairwise(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalencia con distancia euclídea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_euclidea = DistanceMetric.get_metric('minkowski', p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1, 2],\n",
    "    [6, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 7.07106781],\n",
       "       [7.07106781, 0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El resultado es idéntico al obtenido utilizando la métrica de la distancia euclídea\n",
    "dist_euclidea.pairwise(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN en Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El módulo sklearn.neighbors contiene métodos de aprendizaje tanto supervisado como no supervisado:\n",
    "- El algoritmo KNN no supervisado permite encontrar los vecinos más cercanos a un punto dado. Es la base de muchos otros métodos de aprendizaje, especialmente el aprendizaje múltiple (*manifold learning*) y la agrupación espectral (*spectral clustering*) . \n",
    "- El KNN supervisado permite, no solo encontrar a los vecinos más cercanos a un punto, sino también predecir la etiqueta de dicho punto en base a las etiquetas de sus vecinos. Viene en dos versiones: para problemas de clasificación (datos con etiquetas discretas) y para problemas de regresión (datos con etiquetas continuas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecinos más cercanos sin supervisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del módulo sklearn.neighbors, la clase *NearestNeighbors* es la que implementa el modelo de vecinos más cercanos sin supervisión. Actúa como una interfaz uniforme a tres diferentes algoritmos de vecinos más próximos: \n",
    "- Un algoritmo de fuerza bruta\n",
    "- KDTree \n",
    "- BallTree\n",
    "\n",
    "Para especificar el uso de uno de estos algoritmos, la clase *NearestNeighbors* tiene el parámetro *algorithm*. Además, algunos algoritmos se puede utilizar también de forma independiente, ya que scikit-learn cuenta con clases separadas para ellos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo de fuerza bruta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La implementación de búsqueda de vecinos más simple implica el cálculo por fuerza bruta de las distancias entre todos los pares de puntos en el conjunto de datos. Evidentemente, cuantas más dimensiones haya en el conjunto de datos, menos eficiente es esta técnica, llegando a ser incluso inviable. \n",
    "\n",
    "Este algoritmo se especifica utilizando el parámetro algorithm = 'brute'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo KDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para abordar los problemas de coste computacional del enfoque de fuerza bruta, se han inventado una variedad de estructuras de datos basadas en árboles. En general, estas estructuras intentan reducir el número requerido de cálculos de distancia. La idea básica detrás es que si el punto A está muy lejos del punto B, y el punto B está muy cerca del punto C, entonces sabemos que A  y C son muy distantes el uno del otro, sin tener que calcular explícitamente su distancia. De esta manera, el coste computacional de una búsqueda de vecinos más cercanos se puede reducir.\n",
    "\n",
    "Un enfoque en esta línea es el de la estructura de datos del árbol KD (abreviatura de árbol K-dimensional), que generaliza árboles a un número arbitrario de dimensiones. El árbol KD es una estructura de árbol binario que divide de forma recursiva el espacio de parámetros a lo largo de los ejes de datos. Aunque es muy eficaz en bajas dimensiones, también se vuelve ineficaz a medida que crece el número de dimensiones, debido a la \"maldición de la dimensionalidad\". \n",
    "\n",
    "En scikit-learn, este algoritmo se especifica mediante el parámetro algorithm = 'kd_tree'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para abordar las ineficiencias de los árboles KD en dimensiones más altas, se desarrolló el algoritmo BallTree. Donde los árboles KD dividen los datos a lo largo de ejes cartesianos, los árboles de bolas dividen los datos en una serie de hiper-esferas anidadas. Esto hace que la construcción del árbol sea más costosa que la del árbol KD, pero da como resultado una estructura de datos que puede ser muy eficiente en datos altamente estructurados, incluso en dimensiones muy altas.\n",
    "\n",
    "En scikit-learn, las búsquedas de vecinos basadas en árboles de bolas se especifican mediante el parámetro algorithm = 'ball_tree'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecinos más cercanos con supervisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas técnicas permiten encontrar los vecinos más cercanos a un punto dado y, en base a las etiquetas de dichos vecinos, predecir la etiqueta del punto. Por tanto:\n",
    "\n",
    "- Si las etiquetas son discretas, el problema es de clasificación.\n",
    "- Si las etiquetas son continuas, el problema es de regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los modelos de vecinos más cercanos, el número de muestras utilizadas para hacer las predicciones puede ser una constante definida por el usuario (k, que hace referencia al número de vecinos más cercanos) o variar según la densidad local de puntos (búsqueda de vecinos basada en el radio r).  Así, dentro del módulo sklearn.neighbors, Scikit-learn implementa dos clasificadores de vecinos más cercanos diferentes: \n",
    "\n",
    "- La clase *KNeighborsClassifier* implementa el aprendizaje basado en los k vecinos más cercanos. Es la técnica más utilizada. \n",
    "**Nota importante**: si dos vecinos *k+1* y *k* tienen distancias idénticas pero etiquetas diferentes, el resultado dependerá del orden de los datos de entrenamiento.\n",
    "- La clase *RadiusNeighborsClassifier* implementa el aprendizaje basado en el número de vecinos dentro de un radio fijo. Este método es más apropiado en los casos en los que los datos no se muestrean de manera uniforme. El usuario especifica un radio fijo, de modo que los puntos en vecindarios más dispersos usan menos vecinos más cercanos para la clasificación. Para espacios de parámetros de alta dimensión, este método se vuelve menos efectivo debido a la llamada \"maldición de la dimensionalidad\".\n",
    "\n",
    "En ambos casos, la etiqueta de clasificación se calcula a partir de un voto de mayoría simple de los vecinos más cercanos de cada punto: a un punto nuevo se le asigna la clase de datos que tiene más representantes dentro de los vecinos más cercanos del punto. Sin embargo, en algunas circunstancias puede ser mejor ponderar a los vecinos de modo que los vecinos más cercanos contribuyan más al ajuste. Esto se puede ajustar mediante el parámetro \"weights\":\n",
    "- El valor predeterminado weights = 'uniform' asigna pesos uniformes a cada vecino. \n",
    "- El valor weights = 'distance' asigna pesos proporcionales a la inversa de la distancia desde el punto de consulta. \n",
    "\n",
    "Alternativamente, se puede proporcionar una función definida por el usuario de la distancia para calcular los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problemas de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La regresión basada en vecinos se puede utilizar en los casos en que las etiquetas de datos son variables continuas en lugar de discretas. La etiqueta asignada a un punto de consulta se calcula en función de la media de las etiquetas de sus vecinos más cercanos.\n",
    "\n",
    "Igual que en el caso de clasificación, el número de muestras utilizadas para hacer las predicciones puede venir dado por *k* o por *r*. Así, dentro del módulo sklearn.neighbors, Scikit-learn implementa dos regresores de vecinos más cercanos diferentes:\n",
    "\n",
    "- La clase *KNeighborsRegressor* implementa el aprendizaje basado en los k vecinos más cercanos. \n",
    "- La clase RadiusNeighborsRegressor implementa el aprendizaje basado en el número de vecinos dentro de un radio fijo.\n",
    "\n",
    "La regresión básica de vecinos más cercanos utiliza ponderaciones uniformes: es decir, cada punto de la vecindad local contribuye de manera uniforme a la predicción de un punto de consulta. En algunas circunstancias, puede ser útil ponderar las muestras de manera que los puntos cercanos contribuyan más a la regresión que los puntos lejanos. Esto se puede ajustar mediante el parámetro \"weights\":\n",
    "- El valor predeterminado weights = 'uniform' asigna pesos iguales a cada vecino.\n",
    "- El valor weights = 'distance' asigna pesos proporcionales a la inversa de la distancia desde el punto de consulta.\n",
    "\n",
    "Alternativamente, se puede proporcionar una función definida por el usuario de la distancia para calcular los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo basados en k vecinos más cercanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar los parámetros más importantes, veamos la sintaxis de las clases que nos interesan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *n_neighbors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el parámetro que se utiliza en los algoritmos que buscan los k vecinos más cercanos. La elección óptima del valor de n_neighbors depende en gran medida de los datos: en general, un mayor valor suprime los efectos del ruido, pero hace que los límites de clasificación sean menos distintos.\n",
    "\n",
    "- Si k=1, las instancias que son ruido (es decir, aquellas que provocan solape entre clases) ejercen una gran influencia. \n",
    "- Si k>1, se tienen en cuenta más vecinos y las instancias ruidosas pierden influencia.\n",
    "- Si k es muy grande, se pierde la idea principal de localidad del algoritmo. \n",
    "\n",
    "En general:\n",
    "- Cuanto menor es k, mayor es la varianza (se consigue una menor estabilidad).\n",
    "- Cuanto mayor es k, mayor es el bias (se consigue una menor precisión)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *weights*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la función de peso utilizada en la predicción. Puede ser una de las siguientes opciones:\n",
    "\n",
    "{‘uniform’, ‘distance’} o callable\n",
    "\n",
    "Por defecto es 'uniform'.\n",
    "\n",
    "- 'uniforme': pesos uniformes. Todos los puntos de cada vecindario se ponderan por igual.\n",
    "- 'distance': pesos en base a la inversa de la distancia. En este caso, se pondera de tal forma que los vecinos más cercanos de un punto dado tendrán una mayor influencia que los vecinos más alejados.\n",
    "- *callable* (invocable): una función definida por el usuario que acepta una matriz de distancias y devuelve una matriz de la misma forma que contiene los pesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *algorithm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el algoritmo utilizado para calcular los vecinos más cercanos. Puede ser una de las siguientes opciones:\n",
    "\n",
    "{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}\n",
    "\n",
    "Por defecto es ’auto’.\n",
    "\n",
    "- 'ball_tree' usará BallTree.\n",
    "- 'kd_tree' usará KDTree.\n",
    "- 'brute' utilizará una búsqueda de fuerza bruta.\n",
    "- 'auto' intentará decidir el algoritmo más apropiado según los valores pasados método *fit* del modelo.\n",
    "\n",
    "Nota: si se detecta que los datos son disperos (*sparse*), se anulará la configuración de este parámetro, utilizando por defecto la fuerza bruta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *leaf_size*\n",
    "\n",
    "El tamaño de la hoja que se le pasa a los algoritmos BallTree o KDTree. Según el valor que se le dé, se puede ver afectada la velocidad de construcción y consulta, así como la memoria requerida para almacenar el árbol. El valor óptimo depende de la naturaleza del problema.\n",
    "\n",
    "Debe ser un número entero y por defecto se fija en 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *p* \n",
    "\n",
    "Es el parámetro de potencia para la métrica de Minkowski. \n",
    "\n",
    "Debe ser un número entero, que por defecto se fija en 2.\n",
    "\n",
    "- Cuando p = 1, equivale a usar manhattan_distance (l1). \n",
    "- Cuando p = 2, equivale a usar euclidean_distance (l2). \n",
    "- Para p arbitrario, se usa minkowski_distance (l_p)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *metric*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es la métrica de distancia que se utilizará.\n",
    "\n",
    "Debe ser un string o un *callable* (invocable), y por defecto se fija en 'minkowski'.\n",
    "\n",
    "Con p = 2 es equivalente a la métrica euclidiana estándar. En la documentación de DistanceMetric de Scikit-Learn hay una lista de métricas disponibles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *metric_params*\n",
    "\n",
    "A través de este parámetro se pueden pasar parámetros adicionales que necesite la función métrica.\n",
    "\n",
    "Debe ser un objeto de tipo diccionario, por defecto está fijo en \"Ninguno\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo basados en un radio de vecinos más cercanos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para analizar los parámetros más importantes, veamos la sintaxis de las clases que nos interesan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neighbors.RadiusNeighborsClassifier(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, n_jobs=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class sklearn.neighbors.RadiusNeighborsRegressor(radius=1.0, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el parámetro que se utiliza en los algoritmos que buscan los vecinos más cercanos dentro de un radio especificado por el usuario. \n",
    "\n",
    "Debe ser un número en coma flotante, y el valor predeterminado es 1.0.\n",
    "\n",
    "En este caso, los puntos en vecindarios más dispersos usan menos vecinos para la clasificación. Para espacios de parámetros de alta dimensión, este método se vuelve menos efectivo debido a la llamada \"maldición de la dimensionalidad\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparámetro *outlier_label*\n",
    "\n",
    "Es la etiqueta que se desea dar a las muestras atípicas (muestras sin vecinos en un radio determinado). Se puede especificar mediante una de las siguientes opciones:\n",
    "\n",
    "Debe ser una de las siguientes opciones: \n",
    "\n",
    "{manual label, ‘most_frequent’}, default=None\n",
    "\n",
    "El valor predeterminado es None.\n",
    "\n",
    "- *Manual label* (etiqueta manual): debe ser un string o un número entero (del mismo tipo que sea y), o bien una lista de etiquetas manuales si se utiliza salida múltiple.\n",
    "- 'most_frequent': asigna la etiqueta más frecuente de y a los valores atípicos.\n",
    "- *None* (ninguno): cuando se detecta algún valor atípico, se generará ValueError."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resto de hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son iguales que los de los modelos basados en k vecinos más cercanos."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
